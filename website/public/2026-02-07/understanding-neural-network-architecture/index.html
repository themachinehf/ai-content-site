<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Understanding Neural Network Architecture | THE MACHINE</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=VT323&display=swap');
        
        * { margin: 0; padding: 0; box-sizing: border-box; }
        
        body {
            font-family: 'VT323', monospace;
            background: #0a0a0a;
            color: #00ff00;
            min-height: 100vh;
            line-height: 1.6;
            font-size: 18px;
            image-rendering: pixelated;
        }
        
        .container {
            max-width: 720px;
            margin: 0 auto;
            padding: 40px 24px;
        }
        
        .back {
            margin-bottom: 32px;
        }
        
        .back a {
            color: #006600;
            font-size: 0.95rem;
            text-decoration: none;
            padding: 8px 12px;
            border: 1px solid #1a3a1a;
            display: inline-block;
        }
        
        .back a:hover {
            color: #00ff00;
            border-color: #00ff00;
            background: rgba(0, 255, 0, 0.05);
        }
        
        h1 {
            font-family: 'VT323', monospace;
            font-size: 2rem;
            font-weight: 400;
            letter-spacing: 2px;
            color: #00ff00;
            margin-bottom: 12px;
            line-height: 1.3;
        }
        
        .meta {
            color: #006600;
            font-size: 0.85rem;
            margin-bottom: 40px;
            padding-bottom: 20px;
            border-bottom: 1px solid #1a3a1a;
        }
        
        .content {
            font-size: 1rem;
        }
        
        .content h2 {
            font-size: 1.4rem;
            font-weight: 400;
            color: #00ff00;
            margin: 36px 0 16px;
            letter-spacing: 1px;
            border-left: 3px solid #00ff00;
            padding-left: 12px;
        }
        
        .content h3 {
            font-size: 1.15rem;
            font-weight: 400;
            color: #00cc00;
            margin: 28px 0 12px;
        }
        
        .content p {
            margin-bottom: 16px;
            color: #00cc00;
        }
        
        .content a {
            color: #00ff00;
            text-decoration: none;
            border-bottom: 1px dashed #004400;
        }
        
        .content a:hover {
            background: rgba(0, 255, 0, 0.1);
            border-bottom-style: solid;
        }
        
        .content code {
            background: #0d1a0d;
            padding: 2px 8px;
            border-radius: 2px;
            font-size: 0.9em;
            color: #00ff00;
            border: 1px solid #1a3a1a;
        }
        
        .content pre {
            background: #050a05;
            padding: 20px;
            border-radius: 0;
            overflow-x: auto;
            margin: 24px 0;
            border: 1px solid #1a3a1a;
        }
        
        .content pre code {
            background: none;
            padding: 0;
            border: none;
            color: #00ff00;
        }
        
        .content ul, .content ol {
            padding-left: 24px;
            margin: 16px 0;
        }
        
        .content li {
            margin: 10px 0;
            color: #00cc00;
            list-style-type: square;
        }
        
        .content li::marker {
            color: #006600;
        }
        
        .content blockquote {
            border-left: 3px solid #006600;
            margin: 24px 0;
            padding: 16px 20px;
            background: #050a05;
            color: #008800;
        }
        
        .content hr {
            border: none;
            border-top: 1px solid #1a3a1a;
            margin: 40px 0;
        }
        
        .content img {
            max-width: 100%;
            border: 1px solid #1a3a1a;
            margin: 24px 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <nav class="back">
            <a href="/">< RETURN</a>
        </nav>
        
        <article>
            <h1>Understanding Neural Network Architecture</h1>
            <p class="meta">> TIMESTAMP: 2026-02-07 // STATUS: ARCHIVED</p>
            
            <div class="content">
<h1>Understanding Neural Network Architecture</h1>
<p>Neural networks form the foundation of modern AI. This guide explores various architectures and their applications.</p>
<h2>From Perceptrons to Deep Learning</h2>
<h3>The Perceptron</h3>
<p>The perceptron is the simplest neural network unit:</p>
<pre><code class="language-python">import numpy as np

class Perceptron:
    def __init__(self, learning_rate=0.01, epochs=100):
        self.lr = learning_rate
        self.epochs = epochs
    
    def fit(self, X, y):
        self.weights = np.zeros(X.shape[1])
        self.bias = 0
        
        for _ in range(self.epochs):
            for xi, yi in zip(X, y):
                prediction = self.predict(xi)
                update = self.lr * (yi - prediction)
                self.weights += update * xi
                self.bias += update
    
    def predict(self, X):
        return np.where(np.dot(X, self.weights) + self.bias &gt; 0, 1, -1)
</code></pre>
<h3>Multi-Layer Perceptrons (MLP)</h3>
<p>MLPs add hidden layers between input and output:</p>
<pre><code class="language-python">class MLP:
    def __init__(self, layer_sizes, activation=&#39;relu&#39;):
        self.weights = []
        self.biases = []
        self.activation = activation
        
        for i in range(len(layer_sizes) - 1):
            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01
            b = np.zeros((1, layer_sizes[i+1]))
            self.weights.append(w)
            self.biases.append(b)
    
    def relu(self, x):
        return np.maximum(0, x)
    
    def relu_derivative(self, x):
        return (x &gt; 0).astype(float)
    
    def forward(self, X):
        self.activations = [X]
        for i in range(len(self.weights)):
            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]
            if i &lt; len(self.weights) - 1:
                a = self.relu(z)
            else:
                a = z  # No activation on output
            self.activations.append(a)
        return a
</code></pre>
<h2>Convolutional Neural Networks (CNNs)</h2>
<p>CNNs excel at image processing through convolution operations:</p>
<pre><code class="language-python">import torch
import torch.nn as nn

class CNN(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        
        # Convolutional layers
        self.conv1 = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )
        
        self.conv2 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )
        
        self.conv3 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )
        
        # Fully connected layers
        self.fc = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(128 * 4 * 4, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x
</code></pre>
<h2>Recurrent Neural Networks (RNNs)</h2>
<p>RNNs process sequential data with hidden state:</p>
<pre><code class="language-python">import tensorflow as tf
from tensorflow.keras import layers

class LSTMModel(tf.keras.Model):
    def __init__(self, vocab_size, embedding_dim, rnn_units, num_classes):
        super().__init__()
        
        self.embedding = layers.Embedding(vocab_size, embedding_dim)
        
        self.lstm1 = layers.Bidirectional(
            layers.LSTM(rnn_units, return_sequences=True)
        )
        self.lstm2 = layers.Bidirectional(
            layers.LSTM(rnn_units)
        )
        
        self.dropout = layers.Dropout(0.3)
        self.dense = layers.Dense(rnn_units, activation=&#39;relu&#39;)
        self.classifier = layers.Dense(num_classes, activation=&#39;softmax&#39;)
    
    def call(self, inputs, training=False):
        x = self.embedding(inputs)
        x = self.lstm1(x)
        x = self.lstm2(x)
        x = self.dropout(x, training=training)
        x = self.dense(x)
        return self.classifier(x)
</code></pre>
<h2>Transformer Architecture</h2>
<p>Transformers use self-attention for sequence modeling:</p>
<pre><code class="language-python">import math
import torch
import torch.nn as nn

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
    
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        attention = torch.softmax(scores, dim=-1)
        return torch.matmul(attention, V)
    
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        attention = self.scaled_dot_product_attention(Q, K, V, mask)
        attention = attention.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        
        return self.W_o(attention)

class TransformerBlock(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.attention = MultiHeadAttention(d_model, num_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout)
        )
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, mask=None):
        attn_output = self.attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        ff_output = self.feed_forward(x)
        x = self.norm2(x + ff_output)
        return x

class Transformer(nn.Module):
    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, num_classes, dropout=0.1):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = nn.Parameter(torch.randn(1, 1000, d_model) * 0.01)
        self.layers = nn.ModuleList([
            TransformerBlock(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)
        ])
        self.classifier = nn.Linear(d_model, num_classes)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, mask=None):
        x = self.embedding(x) + self.pos_encoding[:, :x.size(1), :]
        x = self.dropout(x)
        for layer in self.layers:
            x = layer(x, mask)
        return self.classifier(x[:, 0, :])  # Use [CLS] token
</code></pre>
<h2>Choosing the Right Architecture</h2>
<table>
<thead>
<tr>
<th>Task</th>
<th>Recommended Architecture</th>
</tr>
</thead>
<tbody><tr>
<td>Image Classification</td>
<td>CNN (ResNet, EfficientNet)</td>
</tr>
<tr>
<td>Text Classification</td>
<td>Transformer (BERT, RoBERTa)</td>
</tr>
<tr>
<td>Time Series</td>
<td>LSTM, Transformer</td>
</tr>
<tr>
<td>Object Detection</td>
<td>CNN + Detection Head (YOLO)</td>
</tr>
<tr>
<td>Machine Translation</td>
<td>Transformer (Encoder-Decoder)</td>
</tr>
<tr>
<td>Image Generation</td>
<td>Diffusion, GAN</td>
</tr>
<tr>
<td>Audio Processing</td>
<td>CNN + LSTM / Transformer</td>
</tr>
</tbody></table>
<h2>Best Practices</h2>
<ol>
<li><strong>Start Simple</strong>: Begin with basic architectures before trying complex ones</li>
<li><strong>Use Pre-trained Models</strong>: Leverage transfer learning when possible</li>
<li><strong>Monitor Training</strong>: Watch for overfitting and underfitting</li>
<li><strong>Hyperparameter Tuning</strong>: Use proper search strategies</li>
<li><strong>Hardware Considerations</strong>: Match architecture to available resources</li>
</ol>
<h2>Conclusion</h2>
<p>Understanding neural network architectures is crucial for building effective AI systems. Each architecture has strengths for specific tasks, and the field continues to evolve with new innovations.</p>

            </div>
        </article>
    </div>
</body>
</html>