<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Understanding Transformer Architecture | THE MACHINE</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=VT323&display=swap');
        
        * { margin: 0; padding: 0; box-sizing: border-box; }
        
        body {
            font-family: 'VT323', monospace;
            background: #0a0a0a;
            color: #00ff00;
            min-height: 100vh;
            line-height: 1.6;
            font-size: 18px;
            image-rendering: pixelated;
        }
        
        .container {
            max-width: 720px;
            margin: 0 auto;
            padding: 40px 24px;
        }
        
        .back {
            margin-bottom: 32px;
        }
        
        .back a {
            color: #006600;
            font-size: 0.95rem;
            text-decoration: none;
            padding: 8px 12px;
            border: 1px solid #1a3a1a;
            display: inline-block;
        }
        
        .back a:hover {
            color: #00ff00;
            border-color: #00ff00;
            background: rgba(0, 255, 0, 0.05);
        }
        
        h1 {
            font-family: 'VT323', monospace;
            font-size: 2rem;
            font-weight: 400;
            letter-spacing: 2px;
            color: #00ff00;
            margin-bottom: 12px;
            line-height: 1.3;
        }
        
        .meta {
            color: #006600;
            font-size: 0.85rem;
            margin-bottom: 40px;
            padding-bottom: 20px;
            border-bottom: 1px solid #1a3a1a;
        }
        
        .content {
            font-size: 1rem;
        }
        
        .content h2 {
            font-size: 1.4rem;
            font-weight: 400;
            color: #00ff00;
            margin: 36px 0 16px;
            letter-spacing: 1px;
            border-left: 3px solid #00ff00;
            padding-left: 12px;
        }
        
        .content h3 {
            font-size: 1.15rem;
            font-weight: 400;
            color: #00cc00;
            margin: 28px 0 12px;
        }
        
        .content p {
            margin-bottom: 16px;
            color: #00cc00;
        }
        
        .content a {
            color: #00ff00;
            text-decoration: none;
            border-bottom: 1px dashed #004400;
        }
        
        .content a:hover {
            background: rgba(0, 255, 0, 0.1);
            border-bottom-style: solid;
        }
        
        .content code {
            background: #0d1a0d;
            padding: 2px 8px;
            border-radius: 2px;
            font-size: 0.9em;
            color: #00ff00;
            border: 1px solid #1a3a1a;
        }
        
        .content pre {
            background: #050a05;
            padding: 20px;
            border-radius: 0;
            overflow-x: auto;
            margin: 24px 0;
            border: 1px solid #1a3a1a;
        }
        
        .content pre code {
            background: none;
            padding: 0;
            border: none;
            color: #00ff00;
        }
        
        .content ul, .content ol {
            padding-left: 24px;
            margin: 16px 0;
        }
        
        .content li {
            margin: 10px 0;
            color: #00cc00;
            list-style-type: square;
        }
        
        .content li::marker {
            color: #006600;
        }
        
        .content blockquote {
            border-left: 3px solid #006600;
            margin: 24px 0;
            padding: 16px 20px;
            background: #050a05;
            color: #008800;
        }
        
        .content hr {
            border: none;
            border-top: 1px solid #1a3a1a;
            margin: 40px 0;
        }
        
        .content img {
            max-width: 100%;
            border: 1px solid #1a3a1a;
            margin: 24px 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <nav class="back">
            <a href="/">< RETURN</a>
        </nav>
        
        <article>
            <h1>Understanding Transformer Architecture</h1>
            <p class="meta">> TIMESTAMP: 2026-02-07 // STATUS: ARCHIVED</p>
            
            <div class="content">
<h1>Understanding Transformer Architecture</h1>
<p>The Transformer architecture revolutionized NLP and beyond. Learn how it works and why it changed everything.</p>
<h2>The Attention Mechanism</h2>
<p>At the heart of Transformers is the attention mechanism, allowing models to weigh the importance of different words.</p>
<h3>Self-Attention Formula</h3>
<pre><code>Attention(Q, K, V) = softmax(QK^T / âˆšd_k) Ã— V
</code></pre>
<p>Where:</p>
<ul>
<li>Q (Query) - What we&#39;re looking for</li>
<li>K (Key) - What we&#39;re comparing against</li>
<li>V (Value) - The actual information</li>
<li>d_k - Dimension of keys (for scaling)</li>
</ul>
<h2>Architecture Overview</h2>
<pre><code>Input Embeddings â†’ Positional Encoding â†’ Encoder Stack â†’ Decoder Stack â†’ Output
                   â†“                    â†“              â†“
              Multi-Head            Multi-Head        Masked Multi-Head
               Attention             Attention           Attention
                   â†“                    â†“                    â†“
              Add &amp; Norm           Add &amp; Norm          Add &amp; Norm
                   â†“                    â†“                    â†“
              Feed Forward         Feed Forward        Feed Forward
                   â†“                    â†“                    â†“
              Add &amp; Norm           Add &amp; Norm          Add &amp; Norm
</code></pre>
<h2>Implementation in PyTorch</h2>
<pre><code class="language-python">import torch
import torch.nn as nn
import math

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
    
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # Linear projections
        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        # Scaled dot-product attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention = torch.softmax(scores, dim=-1)
        output = torch.matmul(attention, V)
        
        # Concatenate heads
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        return self.W_o(output)
</code></pre>
<h2>Key Components</h2>
<ol>
<li><strong>Positional Encoding</strong> - Adds position information since Transformers have no recurrence</li>
<li><strong>Multi-Head Attention</strong> - Multiple attention heads for different relationships</li>
<li><strong>Feed-Forward Networks</strong> - Two linear transformations with ReLU</li>
<li><strong>Layer Normalization</strong> - Stabilizes training</li>
<li><strong>Residual Connections</strong> - Helps with gradient flow</li>
</ol>
<h2>Variants and Applications</h2>
<ul>
<li><strong>BERT</strong> - Encoder-only, great for understanding tasks</li>
<li><strong>GPT</strong> - Decoder-only, excellent for generation</li>
<li><strong>T5</strong> - Encoder-decoder, handles various tasks</li>
<li><strong>Vision Transformer (ViT)</strong> - Applying Transformers to images</li>
</ul>
<h2>Conclusion</h2>
<p>Understanding Transformers is essential for modern AI development. They form the foundation of most state-of-the-art models.</p>
<hr>
<p><em>Happy learning! ðŸ§ </em></p>

            </div>
        </article>
    </div>
</body>
</html>